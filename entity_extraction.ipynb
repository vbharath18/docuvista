{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing_extensions import List, Optional\n",
    "from datetime import date\n",
    "import gradio as gr\n",
    "from pydantic import BaseModel, Field, ValidationInfo, field_validator\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions for semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Functions\n",
    "\n",
    "def process_markdown_for_embeddings():\n",
    "    \"\"\"Process Markdown file for embedding using langchain components\"\"\"\n",
    "    file_path = \"./data/ocr.md\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_text = f.read()\n",
    "        \n",
    "        # Create text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        \n",
    "        # Split text into chunks\n",
    "        texts = text_splitter.create_documents([full_text])\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing Markdown for embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def setup_rag(document_splits=None):\n",
    "    \"\"\"Initialize RAG components with document embedding using FAISS\"\"\"\n",
    "    global vector_store  # Add this line to modify the global variable\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=\"text-embedding-ada-002\",\n",
    "        openai_api_version=\"2023-05-15\",\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=azure_openai_api_key,\n",
    "    )\n",
    "    \n",
    "    # Initialize or load FAISS vector store\n",
    "    if document_splits:\n",
    "        vector_store = FAISS.from_documents(document_splits, embeddings)\n",
    "        # Optionally save the index\n",
    "        vector_store.save_local(\"./data/faiss_index\")\n",
    "    else:\n",
    "        # Load existing index if available\n",
    "        try:\n",
    "            vector_store = FAISS.load_local(\"./data/faiss_index\", embeddings)\n",
    "        except:\n",
    "            # Return None or handle the case when no index exists\n",
    "            return None\n",
    "    return vector_store\n",
    "\n",
    "def is_vector_store_initialized():\n",
    "    \"\"\"Check if the vector store is initialized.\"\"\"\n",
    "    return vector_store is not None\n",
    "\n",
    "def semantic_search(query, k, filter=None):\n",
    "    \"\"\"Perform semantic search from the vector store to retrieve relevant chunks\"\"\"\n",
    "    if not is_vector_store_initialized():\n",
    "        logging.error(\"Vector store is not initialized.\")\n",
    "        return None\n",
    "    \n",
    "    results = vector_store.similarity_search(query, k=k, filter=filter)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Markdown for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='<figure>\\n</figure>\\n\\n\\nMC-2202\\n\\n\\n<figure>\\n\\nsterling\\nACCURIS\\nPathology lab that cares\\n\\n</figure>'),\n",
       " Document(metadata={}, page_content='</figure>\\n\\n\\nScan QR code to check\\nreport authenticity\\n\\nPassport No :\\n\\n\\n# LABORATORY TEST REPORT'),\n",
       " Document(metadata={}, page_content='<table>\\n<tr>\\n<th>Patient Information</th>\\n<th colspan=\"2\">Sample Information</th>'),\n",
       " Document(metadata={}, page_content='<th colspan=\"2\">Sample Information</th>\\n<th>Client/Location Information</th>\\n</tr>\\n<tr>'),\n",
       " Document(metadata={}, page_content='<td rowspan=\"2\">Name : Lyubochka Svetka Sex/Age Male / 41 Y 01-Feb-1982 Ref. Id : Ref. By :</td>')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process Markdown for Embeddings\n",
    "\n",
    "document_splits = process_markdown_for_embeddings()\n",
    "\n",
    "# Display the first few document splits to verify\n",
    "document_splits[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG setup successful.\n"
     ]
    }
   ],
   "source": [
    "# Setup RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "# Initialize RAG components with document embedding using FAISS\n",
    "rag_chain = setup_rag(document_splits)\n",
    "\n",
    "# Check if the RAG setup was successful\n",
    "if rag_chain:\n",
    "    print(\"RAG setup successful.\")\n",
    "else:\n",
    "    print(\"RAG setup failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Vector Store Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Vector Store Initialization\n",
    "\n",
    "# Check if the vector store is initialized\n",
    "is_initialized = is_vector_store_initialized()\n",
    "\n",
    "# Display the initialization status\n",
    "is_initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a575cb8b-5443-4e5d-a1a8-f06fcbe0e9ca', metadata={}, page_content='Dr.Yash Shah\\nMD Path\\n\\n\\\\# Referred Test\\n\\n<!-- PageNumber=\"Page 6 of 19\" -->'),\n",
       " Document(id='3047c552-8b9c-4ca4-a424-f10fe0c9357e', metadata={}, page_content='Dr.Yash Shah\\nMD Path\\n\\n\\\\# Referred Test\\n\\n<!-- PageNumber=\"Page 8 of 19\" -->'),\n",
       " Document(id='ff58991d-b43d-41b2-8d5b-43bdc577a4e7', metadata={}, page_content='Dr.Yash Shah\\nMD Path\\n\\n\\\\# Referred Test\\n\\n<!-- PageNumber=\"Page 3 of 19\" -->'),\n",
       " Document(id='2c3400be-c7b5-4690-8948-0e6e4b578443', metadata={}, page_content='Dr.Yash Shah\\nMD Path\\n\\n\\\\# Referred Test\\n\\n<!-- PageNumber=\"Page 11 of 19\" -->'),\n",
       " Document(id='e9b44ff8-5934-460e-9bcb-a4fe080423e8', metadata={}, page_content='Dr.Yash Shah\\nMD Path\\n\\n\\\\# Referred Test\\n\\n<!-- PageNumber=\"Page 4 of 19\" -->')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Semantic Search\n",
    "\n",
    "# Define the query and perform semantic search\n",
    "query = \"Yash Shah\"\n",
    "results = semantic_search(query, k=5)\n",
    "\n",
    "# Display the search results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Demographics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demographics(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    patient_first_name: Optional[str] = Field(\n",
    "        default=None, description=\"First Name of the patient\"\n",
    "    )\n",
    "   \n",
    "    patient_last_name: Optional[str] = Field(\n",
    "    default=None, description=\"Last Name of the patient\"\n",
    "    )\n",
    "\n",
    "    @field_validator('patient_first_name', 'patient_last_name', mode='after')  \n",
    "    @classmethod\n",
    "    def validate_name(cls, value: str, info: ValidationInfo) -> str:\n",
    "        if not value:\n",
    "            return value\n",
    "        try:\n",
    "            if not is_vector_store_initialized():\n",
    "                return value  # Skip validation if vector store isn't ready\n",
    "            answer = semantic_search(value, k=1)\n",
    "            \n",
    "            print(f\"Validation result for {value}: {answer}\")\n",
    "\n",
    "            if not any(value in result.page_content for result in answer):\n",
    "                print(f\"Warning: Could not verify {value} in the knowledge base\")\n",
    "            return value\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Validation error for {value}: {str(e)}\")\n",
    "            return value\n",
    "\n",
    "    patient_dob: Optional[date] = Field(\n",
    "        default=None, description=\"Date of birth of the patient in YYYY-MM-DD format\"\n",
    "    )\n",
    "    patient_phone: Optional[str] = Field(\n",
    "        default=None, description=\"Phone number of the patient\"\n",
    "    )\n",
    "    patient_address: Optional[str] = Field(\n",
    "        default=None, description=\"Address of the patient\"\n",
    "    )\n",
    "    patient_sex: Optional[str] = Field(\n",
    "        default=None, description=\"Sex of the patient\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Model\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about patient\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Demographics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "\n",
    "if not azure_openai_api_key:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable not set\")\n",
    "if not azure_endpoint:\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT environment variable not set\")\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=azure_deployment_name,  # Add your deployment name here\n",
    "    logprobs=True,\n",
    "    top_logprobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Text for Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Text for Entity Extraction\n",
    "\n",
    "# Initialize structured LLM with the defined schema\n",
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "\n",
    "def process_text(text_input):\n",
    "    try:\n",
    "        # Create a prompt template\n",
    "        prompt_template = PromptTemplate(input_variables=[\"text\"], template=\"{text}\")\n",
    "        prompt = prompt_template.invoke({\"text\": text_input})\n",
    "        \n",
    "        # Invoke the structured LLM with the prompt\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        \n",
    "        # Check if any people data was extracted\n",
    "        if not result.people:\n",
    "            return \"No data was extracted from the text\"\n",
    "        \n",
    "        # Create lists with consistent lengths\n",
    "        data_lists = []\n",
    "        for person in result.people:\n",
    "            person_data = {\n",
    "                \"First Name\": person.patient_first_name or \"\",\n",
    "                \"Last Name\": person.patient_last_name or \"\",\n",
    "                \"Date of Birth\": person.patient_dob or None,\n",
    "                \"Phone\": person.patient_phone or \"\",\n",
    "                \"Address\": person.patient_address or \"\",\n",
    "                \"Sex\": person.patient_sex or \"\"\n",
    "            }\n",
    "            data_lists.append(person_data)\n",
    "        \n",
    "        # Check if any valid data was extracted\n",
    "        if not data_lists:\n",
    "            return \"No valid data extracted\"\n",
    "        \n",
    "        # Create a DataFrame from the extracted data\n",
    "        df = pd.DataFrame(data_lists)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error message for debugging\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        return f\"Error processing the text: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 11:28:48.339 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.403 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/appuser/.local/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-24 11:28:48.403 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.405 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.405 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.406 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.406 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.408 Session state does not function when running a script without `streamlit run`\n",
      "2025-02-24 11:28:48.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.409 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.410 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.410 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 11:28:48.412 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio Interface\n",
    "\n",
    "# Set default text and OCR file path\n",
    "default_text = \"Please input text to extract demographics.\"\n",
    "ocr_file_path = \"./data/ocr.md\"\n",
    "\n",
    "# Try to read the OCR file and set the default text\n",
    "try:\n",
    "    if os.path.exists(ocr_file_path):\n",
    "        with open(ocr_file_path, 'r', encoding='utf-8') as file:\n",
    "            default_text = file.read()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not read OCR file: {str(e)}\")\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_text,\n",
    "    inputs=gr.Textbox(value=default_text, lines=10, label=\"Input Text\"),\n",
    "    outputs=gr.Dataframe(),\n",
    "    title=\"Demographics Extractor\",\n",
    "    description=\"Extract patient demographics from medical documents\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface with sharing enabled\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.0.2.215:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://4.240.39.197:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run /home/appuser/.local/lib/python3.11/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
